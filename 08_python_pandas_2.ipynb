{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I pointed out in the first part of this lesson, tidy data is only useful if we have tools that work with it in a consistent and reproducable manner. One such tools is a `groupby` method of `DataFrame`, which provides a powerful interface to apply any operation based on groupping variables, and we will talk about it in detail in the current section.\n",
    "\n",
    "It turns out that very frequently we need to do some operation based on a groupping variable. A common example is calculating mean of each group (e.g. performance of each subject, or performance on each type of stimuli, etc). This can be thought of as making 3 separate actions:\n",
    "- Splitting the data based on a groupping variable(s)\n",
    "- Applying a function to each group separately\n",
    "- Combining the resulting values back together\n",
    "\n",
    "Based on these 3 actions, this approach is called *Split-Apply-Combine* (SAC) [1].\n",
    "\n",
    "[1] Wickham, Hadley. \"The split-apply-combine strategy for data analysis.\" Journal of Statistical Software 40.1 (2011): 1-29."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/03.08-split-apply-combine.png\"></img>\n",
    "From [\"Aggregation and groupping\" chapter](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb) of [\"Python Data Science Handbook\"](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb) by Jake VanderPlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of operations on data can be thought of as SAC operations. These include calculating sums, means, standard deviations and other parameters of the groups' distributions; transfromations of data, such as normalization or detrending; plotting based on group, e.g. boxplots; and many other. (Some operations cannot be thought of as purely SAC, most prominently those in which data from the same group is used several times, e.g. rolling window means.)\n",
    "\n",
    "A traditional way of doing these operations in include loops, where on each iteration a subset of data is selected and processed. Loops, however, are slow and usually require a lot of code, which makes them difficult to read, and are not easily extendible from 1 to several groupping variables.\n",
    "\n",
    "`Groupby` is a method of `DataFrames` which makes any SAC operation easy to perform and read.\n",
    "\n",
    ">**Note**: Tidy data is the most convenient form for making SAC operations, because you always have access to any combination of your groupping variables due to them being always separated in columns.\n",
    "\n",
    "Let's see a toy example of using a `groupby` operation instead of a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'group': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data': range(6)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I want to calculate a sum of `data` column, based on `group` variable and save it in a `Series`. I can do it with a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.Series()\n",
    "\n",
    "groups = df['group'].unique()\n",
    "for g in groups:\n",
    "    data = df.loc[df['group']==g, 'data']\n",
    "    result[g] = np.sum(data)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code does the job, but it is quite long. If I try to shorten it, it will become very difficult to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.Series()\n",
    "for g in df['group'].unique():\n",
    "    result[g] = np.sum(df.loc[df['group']==g, 'data'])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to do the same thing with `groupby`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('group')['data'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that it is really short and concise and readable. Moreover, let's say I have a more complicated example with several groupping variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'group1': ['A', 'B', 'C']*3,\n",
    "                   'group2': ['A']*4 + ['B']*1 + ['C']*4,\n",
    "                   'data': range(9)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to calculate a sum based on these several groups requires significantly more code with loops. With `groupby` it is as easy as adding another groupping variable in the `groupby` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['group1','group2'])['data'].sum()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Pro-tip**: You may notice that in the resulting `Series` index has 2 levels: `group1` and `group2`. This is referred to as *Hierarchical index* or `MultiIndex`, and is a way to stack several dimensions of data. We won't go much into the details of `MultiIndex` (if you wish to learn more, you may refer to [this section](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.05-Hierarchical-Indexing.ipynb) of [Python Data Science Handbook](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb) and to [MultiIndex](http://pandas.pydata.org/pandas-docs/stable/advanced.html) section of `pandas` documentation. For our purposes we just need to know 2 things: how to index a `MultiIndex` and how to *unstack* dimensions to turn it into a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an element with group1 = A and group2 = C\n",
    "result[('A','C')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack levels of multiindex (turn one of them into a column)\n",
    "result.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, `groupby` is an extremely useful tool for making group-based operations quickly and more readible. Let's see some concrete examples of how you can use it. We will work on the data in the food preferences task provided by Paolo Garlasco. Let's load it first and do some cleanup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Paolo.csv')\n",
    "# drop old index column\n",
    "df.drop('Unnamed: 0', axis='columns', inplace=True)\n",
    "df['cond'].replace({1: 'high vs high', 2: 'low vs low', \n",
    "                    3: 'high vs low', 4: 'low vs high'}, inplace=True)\n",
    "df['congr'].replace({0: 'same', 1: 'different'}, inplace=True)\n",
    "df['session'].replace({0: 'fed', 1: 'hungry'}, inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 4 subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subj_num'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate mean reaction time for each subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('subj_num')['rt'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjects also seem to have more that 1 session, so we might want to compute mean for each session separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_subject_session = df.groupby(['subj_num','session'])['rt'].mean()\n",
    "rt_subject_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='DarkSeaGreen '>Exercise</font>\n",
    "In the cell below calculate mean response for each food item.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, `pandas` provides shortcuts to applying some frequent functions, such as `mean()`, `std()`, `count()`, `min()`, `max()`. However, we can apply any function to the groups. to do that, there are 3 methods: `aggregate()`, `transform()` and `apply()`. Each of these methods require a function (the one you want to apply to the data) as an argument.\n",
    "\n",
    "## Aggregate\n",
    "`aggregate()` can apply any function, which returns a single value for each group (in other words, it *aggregates* a group to a single value). This is what mean, std, count, min, max, and others are. Instead of writing `df.groupby('subj_num')['rt'].mean()` we could've passed a `np.mean` function to calculate means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('subj_num')['rt'].aggregate(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify several functions in a list, and `aggregate()` will return results of all of them in a neat table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('subj_num')['rt'].aggregate([np.mean, np.std, np.median])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly, you can create any function and pass it to `aggregate()` and the function will be applied to each group. The only limitation is that code will assume that the function returns a single value, e.g. calculate half of mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hafl_mean(x):\n",
    "    \"\"\"Calculate half of the mean\"\"\"\n",
    "    mean = np.mean(x)\n",
    "    return mean/2\n",
    "\n",
    "df.groupby('subj_num')['rt'].aggregate(hafl_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "`transform()` works exactly like `aggregate()`, but it expects a function to return a `Series` or an `array` of the same size as input. It will handle the cases when you want to tranform the data. For example, we could subtract the mean reaction time for each subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(x):\n",
    "    return x - np.mean(x)\n",
    "\n",
    "df['rt_minus_mean'] = df.groupby('subj_num')['rt'].transform(subtract_mean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='DarkSeaGreen '>Exercise</font>\n",
    "In the cell below calculate standard score (*z-score*) on reaction time for each subject using `groupby` and `transform`. Save z scores to a new column. \n",
    "\n",
    "See which 10 items require highest reaction times on average in all subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other cases, which don't fall within `aggregate` and `transform` can be handled by `apply` method. In reality, `apply` can act as both `aggregate` and `transform` in most circumstances, but it is slower (because it cannot assume output shape) and cannot do certain things, for example, aggregate several functions at once like `aggregate` method can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping with groupby\n",
    "`groupby`-`apply` combination lets us in general avoid loops, but sometimes you might still need to use them. For example, this can happen when you want to do plotting by group. `groupby` can also simplify that, because it supports iteration through itself. When you do it, on each iteration it will give 2 values: one for the name of the group (basically, groupping variable value) and the values of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a grouby object to a variable\n",
    "groupped = df.groupby('subj_num')['rt']\n",
    "# iterate through groupby object\n",
    "for name, data in groupped:\n",
    "    # groupping variable value\n",
    "    print(name)\n",
    "    # shape of the data: in this case the 'rt' values for each group\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataFrame` and `Series` transformations\n",
    "Now that you know the power of `groupby` and having data in a tidy format, let's talk about how to get there. In general, you should become comfortable with transforming your data to any shape you want, because the tools you might want to use, won't necessarily work with tidy data. `pandas` provides a lot of ways to tranform `Series` and `DataFrame` objects.\n",
    "\n",
    "## `Set`, `reset` index\n",
    "Index is very useful for retrieving values, but also for other things. For example, as we will see in the visualization lesson, when plotting a `Series`, index will be automatically assumed to be the X axis, and the values will become the Y axis. This is useful for quick exploratory visualization.\n",
    "\n",
    "Main methods to interact with the index are `set_index()` and `reset_index()`. First takes a column and makes it into a new index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df.set_index('item')\n",
    "df_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if you say append=True, you can keep the old index too, which will result in a MultiIndex\n",
    "df.set_index('item', append=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reset_index()` will make the old index into a columns and instead create a new index, which has values from `0` to the number of rows minus 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our DataFrame indexed by items\n",
    "df_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reset index\n",
    "df_items.reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two methods make working with index very dynamic -- you can set it and reset it to become a normal column again whenever you need. You can also set several columns (pass them as a list to `set_index`) and create a `MultiIndex`.\n",
    "\n",
    "## Melt\n",
    "The concept of melting is related to tidying the data. `melt` function takes all columns of the `DataFrame` and creates 2 columns from them: one with groupping variable (former name of the column) and another with the value variable. If applied correctly, the resulting *molten* `DataFrame` will be tidy.\n",
    "\n",
    "Let's see a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untidy = pd.DataFrame({'treatment_a':[np.nan, 16, 3],'treatment_b':[2,11,1]})\n",
    "untidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's melt\n",
    "pd.melt(untidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the data is reshaped. What were the names of the columns in the untidy `DataFrame` (`treatment_a` and `treatment_b`) are now the groupping variable. The values inside the table are now all in the single \"value\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also specify the names of the resulting columns\n",
    "pd.melt(untidy, var_name='treatment', value_name='measurement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently you want to melt only certain columns, because some are already groupping variable. Specify them as `id_vars` in the `melt` function and they will not be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example \"person\" is already a separated variable\n",
    "untidy = pd.DataFrame({'treatment_a':[np.nan, 16, 3],'treatment_b':[2,11,1], \n",
    "                      'person':['John Smith', 'Jane Doe','Mary Johnson']})\n",
    "untidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(untidy, id_vars='person', var_name='treatment', value_name='measurement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see another example, taken directly from the [lesson on tidy data](http://nbviewer.jupyter.org/github/antopolskiy/sciprog/blob/master/002_data_organization_00_slides.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_untidy = pd.read_csv(os.path.join('data','pew.csv'))\n",
    "print(income_untidy.shape)\n",
    "income_untidy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case all columns except for `religion` have the same variable (count of people who belongs to this group), so we keep `religion` and melp all other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "income_tidy = pd.melt(income_untidy,id_vars='religion',var_name='income',value_name='count')\n",
    "print(income_tidy.shape)\n",
    "income_tidy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot table\n",
    "Pivoting is another way of transforming the `DataFrames`, which is usually used to tranform a tidy `DataFrame` in some other form. For example, it can be used to undo melting. Using method `pivot_table` is easy: simply think about which column you want to have as and index and which as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# molten dataframe\n",
    "income_tidy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoting to undo melting\n",
    "income_tidy.pivot_table(columns='income', index='religion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But pivoting can achieve much more than that. Let's look at another example. This dataset contains number of births for each day from 1969 to 2008:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "births = pd.read_csv(os.path.join('data','births.csv'))\n",
    "births.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to calculate the total number of births for each year for boys and girls to see how the gender proportions change over the years. We could achieve it with `groupby`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births.groupby(['year','gender'])['births'].sum().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then use `unstack` on the resulting `Series` to create a nice table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births.groupby(['year','gender'])['births'].sum().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot table can do the same and in some cases can be more readable, because when we pivot we don't need to think about groupping, but instead we think about what kind of table we want to get in the end. In this case I think to myself: \"I want *year* to be the index, *genders* will be the columns. I will take the *births* columns and *sum* them up for each resulting group\". The syntax of `pivot_table` repeats this thinking almost exactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_year_gender = births.pivot_table(index='year', columns='gender', values='births', aggfunc=np.sum)\n",
    "births_year_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see another example on Paolo's food preference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to create a table with mean reaction times with rows being session type and columns being the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(values='rt', index='session', columns='cond', aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='DarkSeaGreen '>Exercise</font>\n",
    "Using `births` dataset, create a table in which there would be total number of births for each month for each year. Do it using `groupby-aggregate-unstack` and using `pivot_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='DarkSeaGreen '>Exercise</font>\n",
    "Using food preference dataset, create a table in which the index would be items, columns would be session type and the values would be mean response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# nevermind this part, this is just to display several tables alongside\n",
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this function will quickly create DataFrames for our toy examples\n",
    "\n",
    "def make_df(cols, ind):\n",
    "    \"\"\"Quickly make a DataFrame\"\"\"\n",
    "    data = {c: [str(c) + str(i) for i in ind]\n",
    "            for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n",
    "\n",
    "# example DataFrame\n",
    "make_df('ABCD', range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves **3 principles of tidy data**:\n",
    "- Each variable forms a column\n",
    "- Each observation forms a row\n",
    "- Each type of observation forms a separate table\n",
    "\n",
    "If first 2 priciples are rather easy to digest, the third one at times seems to make life harder rather than easier. In fact, if you split your data into several tables and don't know how to merge it back in a format you need for a certain analysis, you can lose a lot of time. Merging data from different tables can be very daunting if done manually. Data can have diverse type, some rows or columns can be present only in one of the tables, etc. Historically, the problems of merging were addressed by databases, such as SQL. `pandas` provides a lot of functionality in this domain.\n",
    "\n",
    "Besides that, merging is often necessary when tidying data from different sources, for example, you might have a table for each participant and you want to put them all together.\n",
    "\n",
    "# Append\n",
    "There are several distinct types of putting tables together. The easiest one to understand is `append` -- it is a method of `DataFrame` which will take another `DataFrame` and put it directly under the first one, independent of `index` (`index` is preserved from both `DataFrames`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = make_df('ABC',range(3))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append another copy of the same DataFrame\n",
    "df1.append(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`append` will try to match columns. If some columns are present in one `DataFrame` but not in another, it will put missing values where appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns A exists only in df1, column D only in df2\n",
    "df1 = make_df('ABC',range(3))\n",
    "df2 = make_df('BCD',range(3))\n",
    "display('df1','df2','df1.append(df2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`append` is very useful when you want to quickly put together some tables with the same type of data, for example tables for separate subjects. But that's just about its functionality.\n",
    "\n",
    "# Concat\n",
    "`concat` can be thought of as generalized `append`. It can do all the things `append` can and much more. For example, it can take a list of `DataFrames` and put them all together in `append`-like manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df1, df2, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass a list of `keys` to `concat` (same size as list of `DataFrames` to concatenate) and each `DataFrame` will have its own key in the index. This is useful in a situation where you merge several subjects and want to keep each one labeled with which subject this came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df1, df2, df2], keys=['df1','df2','df3','df4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also concatenate along `columns` instead of `index` by passing `axis` argument. In this case `concat` will try to match `index` (as it tried to match columns when you were concatenating along `index`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df1, df1, df2, df2], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a real example of `concat` use. Here I load data from 2 subjects, which is stored in separate *.mat* files. I concatenate them to create a signle table, and I reset the index. Then I name the columns according to the order given me by the person who conducted the experiment. Resulting is one tidy `DataFrame` with 2 subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concat` is useful for any kind of simple concatenation where we have the same type of data in different tables. However, when we have different *types* of observations in different tables, `concat` will fail us.\n",
    "\n",
    "# Merge\n",
    "If merging data sounds confusing to you at any point, it is because it is. There is a whole area of math called *relational algebra*, which creates the theoretical underpinnings of databases and how they work. We won't study any of that, not only it requires a course of its own, but it is also not very frequent for scientists to deal with the kind of data that requires databases.\n",
    "\n",
    "Merging, however, is important if you want to work with tidy data and avoid data duplication (which is not only inefficient, but also invites errors). So we will learn a bit about that.\n",
    "\n",
    "In general there are 3 types of joins: one-to-one, many-to-one, and many-to-many. Pandas function `merge()` provides an interface to do all of them, depending on the inputs. The first (one-to-one) referers to the simplest case, when you have 2 sources, and none of them have duplicate entries. In this case joining is usually easy, and basically reminds a concatenation. Let's see it with a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'department': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "display('df1', 'df2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the 2 tables have a common column `employee`, but the order is different. We want to merge the two tables consistently, to see the hire date for our different departments. In this case `merge` will automatically find the matching column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: if we tried to perform `concat` here with `axis='columns'`, it would match the `index`, but not the `employee`. You could get around it by first setting the `employee` as the index in both tables, and then perform the `concat` on columns. You could then reset the index and get out the same table. But it is inefficient. Nevertheless, let's do it for the sake of demonstation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_e = df1.set_index('employee')\n",
    "df2_e = df2.set_index('employee')\n",
    "df_e = pd.concat([df1_e, df2_e], axis='columns')\n",
    "display(\"df1_e\",\"df2_e\",\"df_e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-to-many is when one of your `DataFrames` contains duplicate entries. `merge` will understand that and try to fill in the values appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'department': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "display('df3','df4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how the supervisor column in the resulting DataFrame has Guido\n",
    "# across from every person in Engineering department\n",
    "df5 = pd.merge(df3, df4)\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many-to-many is the most confusing type of join, but it is nevertheless well defined mathematically. Consider the following, where we have a `DataFrame` showing one or more skills associated with a particular department. By performing a many-to-many join, we can recover the skills associated with any individual person. Note that some entries in both `df1` and `df6` had to be duplicated; also \"R&D\" group disappeared in the joined `DataFrame`, because it had no pairings within `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'department': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR', 'R&D'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization', 'science']})\n",
    "df7 = pd.merge(df1, df6)\n",
    "display('df1', 'df6', 'df7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things need to be pointed out. \n",
    "\n",
    "**First**, when you merge, you can specify a parameter `how`, which can have 1 of 4 values: *left*, *right*, *outer* or *inner*. This controls which values remain in the resulting `DataFrame` if some values are present only in one of the `DataFrames` you're merging. By default `how='inner'`, and it means that the resulting `DataFrame` will have the *intersection* of values from the input `DataFrames`, that is, only values present in both `DataFrames` will be present in the result. That is why we don't have *R&D* in the `df7`: there is no match for it in the `df1`. `outer` is the opposite of `inner` -- all the values will be present in the result. Let's try to do the same merge, but with `how='outer'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.merge(df1, df6, how='outer')\n",
    "display('df1', 'df6', 'df7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how now there is *R&D* in the resulting `DataFrame`, although there is no employee who is in this department.\n",
    "\n",
    "`left` and `right` just say that values from the first or the second of the input `DataFrames` will be used. In this case, if I used `how='left'`, only values from `df1` would be used, and for `how='right'` -- only from `df6`. (*Left* and *right* just refer to their positions as the inputs to the `merge` function; this terminology, as well as *inner* and *outer*, is taken directly from the database systems, otherwise they might as well be named \"first\" and \"second\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**: `merge` will try to infer which column(s) to use in both `DataFrames` to match the data consistently. However, it is safest to specify it manually, then the outcome is most predictable. If you want to use a certain column, specify `left_on` parameter (for the first input `DataFrame`) and `right_on` (for the second one). This is extremely useful for when you have several columns matching names and they are not consistent with one another, and you want the outcome to be 100% predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result is equivalent to what we had before, but we have more control\n",
    "df7 = pd.merge(df1, df6, left_on='department', right_on='department')\n",
    "display('df1', 'df6', 'df7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to use index in one of the `DataFrames` for matching. In this case just specify `left_index=True` instead of `left_on` (same for the `right`). \n",
    "\n",
    "Let's see it with an example. We load and concatenate the data from a vibration experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "s1_mat = loadmat('data/Ale_subj1.mat')\n",
    "s1_df = pd.DataFrame(s1_mat['Subject1'])\n",
    "print('Subject1 df shape:', s1_df.shape)\n",
    "\n",
    "s2_mat = loadmat('data/Ale_subj2.mat')\n",
    "s2_df = pd.DataFrame(s2_mat['Subject2'])\n",
    "print('Subject2 df shape:', s2_df.shape)\n",
    "\n",
    "df_vibr = pd.concat([s1_df, s2_df])\n",
    "df_vibr = df_vibr.reset_index(drop=True)\n",
    "df_vibr.columns = ['id_subj','session','trial','s1_int','s1_dur','s1_seed','inter_stim_delay',\n",
    "              's2_int','s2_dur','s2_seed','pre_stim_delay','task_type','rewarded_choice',\n",
    "              'subj_choice','s1_motor','s2_motor']\n",
    "print('Concatenated df shape:',df_vibr.shape)\n",
    "df_vibr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associated with these data, there is some information about the subjects in the Excel spreadsheet:\n",
    "\n",
    "> When you try to read the `.xlsx` file, you might get an error \"`No module named 'xlrd'`\". This is because `pandas` is using another module to load the Excel file, and you need to install that module. Open your computer's terminal (console or command prompt (\"cmd\") in Windows) and run `pip install xlrd`. Now it should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vibr_subj = pd.read_excel('data/Ale_subj_info.xlsx')\n",
    "df_vibr_subj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to look at, for example, performance depending on gender or age, we would have to merge these 2 tables to perform a `groupby`. If we try to do it by hand, it would be very cumbersom. Instead, let's do it using `merge`. First I want to make the column `id_subj` in the `df_vibr_subj` so that it matches the values in the `df_vibr` (i.e. now we have `Sub1`, but we should have just `1`). Then I make it index of the subjects table. I will also drop some other columns so that the result is clearer (otherwise after merge we will have all the columns there and it might be a bit confusing; also, in this example we just need gender and age, so it makes sense to merge in only these columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column\n",
    "df_vibr_subj.rename(columns={'Subjects':'id_subj'}, inplace=True)\n",
    "# mapping of old values to new (see Pro-tip below)\n",
    "subj_mapping_dict = {name_old:int(name_old[-1]) for name_old in df_vibr_subj['id_subj'].unique()}\n",
    "# replace values based on mapping\n",
    "df_vibr_subj['id_subj'] = df_vibr_subj['id_subj'].replace(subj_mapping_dict)\n",
    "# set id_subj as index\n",
    "df_vibr_subj.set_index('id_subj', inplace=True)\n",
    "df_vibr_subj = df_vibr_subj[['age','Gender']]\n",
    "df_vibr_subj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pro-tip**: This line\n",
    "    \n",
    ">    `subj_mapping_dict = {name_old:int(name_old[-1]) for name_old in df_vibr_subj['id_subj'].unique()}`\n",
    "    \n",
    ">creates a dictionary with mappings for replacing Sub1 with 1, Sub2 with 2, etc; this is called \"dict comprehension\" and is just an extension of list comprehensions to create a dictionary; go here to learn more: https://www.python.org/dev/peps/pep-0274/\n",
    "\n",
    "Now that we have the subject's id in both tables, we can merge. I specify `id_subj` as target merging column for `df_vibr` and `index` for `df_vibr_subj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.merge(df_vibr, df_vibr_subj, left_on='id_subj', right_index=True)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Voil√†!* Now we have age and gender for every trial (last columns) and we could group based on these and calculate some statistics. Note that although in the `df_vibr_subj` we have many more subjects (7 in total), in the `df_vibr` we only have data for 2 subjects. Because the default merge is *inner*, only the data from the 2 subjects present in both tables is present in the final table.\n",
    "\n",
    "Now, for example, we can calculate avegare subject's choice for gender and age (in this case we have only 2 subjects and they are both males, so it doesn't make much sense; but if you had many, it would work flawlessly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.groupby(['Gender','age'])['subj_choice'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='DarkSeaGreen '>Exercise</font>\n",
    "Load 2 tables from the data folder: `toy_subjects.csv` and `toy_scores.csv` (use `read_csv` function with `index_col=0` parameter to make the first column into index). In the `toy_scores` calculate the mean `score` for male and female subjects. To do it, you'll need to merge the 2 tables, then group by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we have only scratched the surface of merging. If you ever need to do complicated joins or are just interested in learning more about joins with `pandas`, I highly recommend <a href=\"http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb\">Combining Datasets: Merge and Join</a> section on the \"Python Data Science Handbook\" by Jake VanderPlas and <a href=\"http://pandas.pydata.org/pandas-docs/stable/merging.html\">Merge, join, and concatenate</a> section of the `Pandas` documentation.\n",
    "\n",
    "Besides, at this point you can go and review the code in the `002_data_organization_00_slides` notebook, because now you should be able to understand everything that is going on there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
